class_labels
View(class_labels)
# LOAD CLASS LABELS
class_labels = read.csv('../class_dict.csv', header = F)
View(class_labels)
# LOAD CLASS LABELS
class_labels = read.csv('../class_dict.csv', header = F, sep = ";")
View(class_labels)
names(class_labels) = c('title', 'id')
substr(as.character(class_labels$title), 1, 2)
substr(as.character(class_labels$title), 1, 1)
class_labels$sponsored = substr(as.character(class_labels$title), 1, 1)
class_labels$Filename = paste(as.character(class_labels$id), 'txt', sep=".")
###############################################################################
##### YOUTUBE ADS DETECTION #####
##### KLEINBERG, MOZES, VAN DER VEGT ###
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(caret)
require(e1071)
require(pROC)
require(tm)
require(data.table)
require(ggplot2)
require(tidyr)
require(MLmetrics)
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
## set dir
setwd('/Users/bennettkleinberg/Documents/Research/CBDMI_Schiphol/youtube_ads/data/dataset/separated')
# LOAD CLASS LABELS
class_labels = read.csv('../class_dict.csv', header = F, sep = ";")
View(class_labels)
names(class_labels) = c('title', 'id')
class_labels$sponsored = substr(as.character(class_labels$title), 1, 1)
class_labels$Filename = paste(as.character(class_labels$id), 'txt', sep=".")
# LOAD DATA
liwc_pos = as.data.frame(fread('liwc_pos.txt'
, header=T))
liwc_neg = as.data.frame(fread('liwc_neg.txt'
, header=T))
sep_pos = txt_df_from_dir(dirpath = './pos'
, include_processed = T)
sep_neg = txt_df_from_dir(dirpath = './neg'
, include_processed = T)
pos = merge(sep_pos, liwc_pos, by = 'Filename')
pos$valence = 'pos'
pos$Filename_val = paste(as.character(pos$Filename), 'pos', sep = "_")
pos = merge(pos, class_labels, by='Filename')
neg = merge(sep_neg, liwc_neg, by = 'Filename')
neg$valence = 'neg'
neg$Filename_val = paste(as.character(neg$Filename), 'neg', sep = "_")
neg = merge(neg, class_labels, by='Filename')
data = rbind(pos, neg)
dim(data)
pos = merge(sep_pos, liwc_pos, by = 'Filename')
pos$valence = 'pos'
pos$Filename_val = paste(as.character(pos$Filename), 'pos', sep = "_")
pos = merge(pos, class_labels, by='Filename')
neg = merge(sep_neg, liwc_neg, by = 'Filename')
neg$valence = 'neutral'
neg$Filename_val = paste(as.character(neg$Filename), 'neg', sep = "_")
neg = merge(neg, class_labels, by='Filename')
data = rbind(pos, neg)
data$valence
data$class = ifelse(data$valence == 'pos' & data$sponsored == 'y', 'PS',
ifelse(data$valence == 'pos' & data$sponsored == 'n', 'PN',
ifelse(data$valence == 'neutral' & data$sponsored == 'yes', 'NS', 'NN')))
View(data[1:20])
View(data[1:20, 90:104])
View(data[100:120, 90:104])
View(data[500:520, 90:104])
data$class = ifelse(data$valence == 'pos' & data$sponsored == 'y', 'PS',
ifelse(data$valence == 'pos' & data$sponsored == 'n', 'PN',
ifelse(data$valence == 'neutral' & data$sponsored == 'y', 'NS', 'NN')))
View(data[500:520, 90:104])
table(data$class)
table(data$valence, data$sponsored)
nrow(data)
# FEATURE EXTRACTION
require(quanteda)
param_sparsity = .95
### unigrams
unigrams = dfm(data$text
, ngrams = 1
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
unigrams = dfm_trim(unigrams, sparsity = param_sparsity)
### bigrams
bigrams = dfm(data$text
, ngrams = 2
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
bigrams = dfm_trim(bigrams, sparsity = param_sparsity)
### trigrams
trigrams = dfm(data$text
, ngrams = 3
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
trigrams = dfm_trim(trigrams, sparsity = param_sparsity)
### bigrams+
bigrams_plus = dfm(data$text
, ngrams = 1:2
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
bigrams_plus = dfm_trim(bigrams_plus, sparsity = param_sparsity)
### trigrams+
trigrams_plus = dfm(data$text
, ngrams = 1:3
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
trigrams_plus = dfm_trim(trigrams_plus, sparsity = param_sparsity)
df_unigrams = as.data.frame(unigrams)
df_unigrams$text_id = row.names(df_unigrams)
## minimal dataframe for merger
data_min = data[, c('class', 'text_id')]
## set merger key
data$text_id = paste('text', row.names(data), sep="")
## minimal dataframe for merger
data_min = data[, c('class', 'text_id')]
data_min
df_unigrams = as.data.frame(unigrams)
df_unigrams$text_id = row.names(df_unigrams)
df_unigrams$text_id
data_unigrams = merge(data_min, df_unigrams, by='text_id')
data_unigrams
unigrams = dfm(data$text
, ngrams = 1
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
unigrams = dfm_trim(unigrams, sparsity = param_sparsity)
as.data.frame(unigrams)
df_unigrams = as.data.frame(unigrams)
df_unigrams$text_id = row.names(df_unigrams)
data_unigrams = merge(data_min, df_unigrams, by='text_id')
names(data_unigrams)
data$outcome_class = ifelse(data$valence == 'pos' & data$sponsored == 'y', 'PS',
ifelse(data$valence == 'pos' & data$sponsored == 'n', 'PN',
ifelse(data$valence == 'neutral' & data$sponsored == 'y', 'NS', 'NN')))
table(data$class)
data = rbind(pos, neg)
data$outcome_class = ifelse(data$valence == 'pos' & data$sponsored == 'y', 'PS',
ifelse(data$valence == 'pos' & data$sponsored == 'n', 'PN',
ifelse(data$valence == 'neutral' & data$sponsored == 'y', 'NS', 'NN')))
table(data$class)
table(data$outcome_class)
table(data$valence, data$sponsored)
nrow(data)
data$text_id = paste('text', row.names(data), sep="")
## minimal dataframe for merger
data_min = data[, c('outcome_class', 'text_id')]
param_sparsity = .95
### unigrams
unigrams = dfm(data$text
, ngrams = 1
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
unigrams = dfm_trim(unigrams, sparsity = param_sparsity)
df_unigrams = as.data.frame(unigrams)
df_unigrams$text_id = row.names(df_unigrams)
data_unigrams = merge(data_min, df_unigrams, by='text_id')
names(data_unigrams)
data_unigrams = data_unigrams[, -1]
bigrams = dfm(data$text
, ngrams = 2
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
bigrams = dfm_trim(bigrams, sparsity = param_sparsity)
df_bigrams = as.data.frame(bigrams)
df_bigrams$text_id = row.names(df_bigrams)
data_bigrams = merge(data_min, df_bigrams, by='text_id')
data_bigrams = data_bigrams[, -1]
trigrams = dfm(data$text
, ngrams = 3
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
trigrams = dfm_trim(trigrams, sparsity = param_sparsity)
df_trigrams = as.data.frame(trigrams)
df_trigrams$text_id = row.names(df_trigrams)
data_trigrams = merge(data_min, df_trigrams, by='text_id')
data_trigrams = data_trigrams[, -1]
bigrams_plus = dfm(data$text
, ngrams = 1:2
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
bigrams_plus = dfm_trim(bigrams_plus, sparsity = param_sparsity)
df_bigrams_plus = as.data.frame(bigrams_plus)
df_bigrams_plus$text_id = row.names(df_bigrams_plus)
data_bigrams_plus = merge(data_min, df_bigrams_plus, by='text_id')
data_bigrams_plus = data_bigrams_plus[, -1]
trigrams_plus = dfm(data$text
, ngrams = 1:3
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
trigrams_plus = dfm_trim(trigrams_plus, sparsity = param_sparsity)
df_trigrams_plus = as.data.frame(trigrams_plus)
df_trigrams_plus$text_id = row.names(df_trigrams_plus)
data_trigrams_plus = merge(data_min, df_trigrams_plus, by='text_id')
data_trigrams_plus = data_trigrams_plus[, -1]
unigram_tfidf = tfidf(unigrams
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_unigram_tfidf = as.data.frame(unigram_tfidf)
df_unigram_tfidf$text_id = row.names(df_unigram_tfidf)
data_unigrams_tfidf = merge(data_min, df_unigram_tfidf, by='text_id')
data_unigrams_tfidf = data_unigrams_tfidf[, -1]
data_unigrams_tfidf
View(data_unigrams_tfidf[1:10, 1:20])
bigram_tfidf = tfidf(bigrams
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_bigram_tfidf = as.data.frame(bigram_tfidf)
df_bigram_tfidf$text_id = row.names(df_bigram_tfidf)
data_bigrams_tfidf = merge(data_min, df_bigram_tfidf, by='text_id')
data_bigrams_tfidf = data_bigrams_tfidf[, -1]
trigram_tfidf = tfidf(trigrams
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_trigram_tfidf = as.data.frame(trigram_tfidf)
df_trigram_tfidf$text_id = row.names(df_trigram_tfidf)
data_trigrams_tfidf = merge(data_min, df_trigram_tfidf, by='text_id')
data_trigrams_tfidf = data_trigrams_tfidf[, -1]
bigram_plus_tfidf = tfidf(bigram_plus
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_bigram_plus_tfidf = as.data.frame(bigram_plus_tfidf)
df_bigram_plus_tfidf$text_id = row.names(df_bigram_plus_tfidf)
data_bigram_plus_tfidf = merge(data_min, df_bigram_plus_tfidf, by='text_id')
data_bigram_plus_tfidf = data_bigram_plus_tfidf[, -1]
bigram_plus_tfidf = tfidf(bigram_plus
, scheme_tf = 'log'
, scheme_df = 'inverse')
bigrams_plus_tfidf = tfidf(bigrams_plus
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_bigrams_plus_tfidf = as.data.frame(bigrams_plus_tfidf)
bigrams_plus_tfidf = tfidf(bigrams_plus
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_bigrams_plus_tfidf = as.data.frame(bigrams_plus_tfidf)
df_bigrams_plus_tfidf$text_id = row.names(df_bigrams_plus_tfidf)
data_bigrams_plus_tfidf = merge(data_min, df_bigrams_plus_tfidf, by='text_id')
data_bigrams_plus_tfidf = data_bigrams_plus_tfidf[, -1]
trigrams_plus_tfidf = tfidf(trigrams_plus
, scheme_tf = 'log'
, scheme_df = 'inverse')
df_trigrams_plus_tfidf = as.data.frame(trigrams_plus_tfidf)
df_trigrams_plus_tfidf$text_id = row.names(df_trigrams_plus_tfidf)
data_trigrams_plus_tfidf = merge(data_min, df_trigrams_plus_tfidf, by='text_id')
data_trigrams_plus_tfidf = data_trigrams_plus_tfidf[, -1]
raw_dfm = dfm(data$text
, ngrams = 1
, verbose = T
, remove_punct = T
, remove = stopwords("english")
, stem = F
)
df_lexdiv = as.data.frame(textstat_lexdiv(raw_dfm, c("TTR")))
df_lexdiv$text_id = row.names(df_lexdiv)
names(df_lexdiv)
names(df_lexdiv) = c('ttr', 'text_id')
data = merge(data, df_lexdiv, by='text_id')
dim(data)
data$WC
data$pos_adj = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'ADJ'
, verbose = T)/data$WC
data$pos_adv = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'ADV'
, verbose = T)/data$WC
data$pos_intj = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'INTJ'
, verbose = T)/data$WC
data$pos_noun = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'NOUN'
, verbose = T)/data$WC
data$pos_propn = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'PROPN'
, verbose = T)/data$WC
data$pos_verb = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'VERB'
, verbose = T)/data$WC
data$pos_adp = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'ADP'
, verbose = T)/data$WC
data$pos_aux = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'AUX'
, verbose = T)/data$WC
data$pos_cconj = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'CCONJ'
, verbose = T)/data$WC
data$pos_cconj
data$Filename
data$pos_det = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'DET'
, verbose = T)/data$WC
data$pos_num = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'NUM'
, verbose = T)/data$WC
data$pos_part = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'PART'
, verbose = T)/data$WC
data$pos_pron = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'PRON'
, verbose = T)/data$WC
data$pos_sconj = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'SCONJ'
, verbose = T)/data$WC
data$pos_punct = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'PUNCT'
, verbose = T)/data$WC
data$pos_sym = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'SYM'
, verbose = T)/data$WC
data$pos_x = get_pos_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, pos_type = 'X'
, verbose = T)/data$WC
data$rdb_fleschkincaid = get_single_readability(data$text, 'Flesch_Kincaid')
data$rdb_gunningfog = get_single_readability(data$text, 'Gunning_Fog_Index')
data$rdb_colemanliau = get_single_readability(data$text, 'Coleman_Liau')
data$rdb_smog = get_single_readability(data$text, 'SMOG')
data$rdb_autoreadindex = get_single_readability(data$text, 'Automated_Readability_Index')
data$rdb_Average_Grade_Level = get_single_readability(data$text, 'Average_Grade_Level')
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./get_single_readability.R')
## set dir
setwd('/Users/bennettkleinberg/Documents/Research/CBDMI_Schiphol/youtube_ads/data/dataset/separated')
data$rdb_fleschkincaid = get_single_readability(data$text, 'Flesch_Kincaid')
data$rdb_gunningfog = get_single_readability(data$text, 'Gunning_Fog_Index')
data$rdb_colemanliau = get_single_readability(data$text, 'Coleman_Liau')
data$rdb_smog = get_single_readability(data$text, 'SMOG')
data$rdb_autoreadindex = get_single_readability(data$text, 'Automated_Readability_Index')
data$rdb_Average_Grade_Level = get_single_readability(data$text, 'Average_Grade_Level')
table(data$rdb_colemanliau, data$Filename)
data$rdb_colemanliau
table(data$Filename,data$rdb_colemanliau)
data[data$Filename == '1.txt', 'rdb_gunningfog']
data$ner_person = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'person'
, verbose = T)/data$WC
data$ner_norp = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'norp'
, verbose = T)/data$WC
data$ner_facility = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'facility'
, verbose = T)/data$WC
data$ner_org = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'org'
, verbose = T)/data$WC
data$ner_gpe = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'gpe'
, verbose = T)/data$WC
data$ner_loc = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'loc'
, verbose = T)/data$WC
data$ner_product = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'product'
, verbose = T)/data$WC
data$ner_event = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'event'
, verbose = T)/data$WC
data$ner_woa = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'work_of_art'
, verbose = T)/data$WC
data$ner_law = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'law'
, verbose = T)/data$WC
data$ner_language = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'language'
, verbose = T)/data$WC
data$ner_date = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'date'
, verbose = T)/data$WC
data$ner_time = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'time'
, verbose = T)/data$WC
data$ner_percent = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'percent'
, verbose = T)/data$WC
data$ner_money = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'money'
, verbose = T)/data$WC
data$ner_quantity = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'quantity'
, verbose = T)/data$WC
data$ner_ordinal = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'ordinal'
, verbose = T)/data$WC
data$ner_cardinal = get_entity_count(df_identifier = data$Filename
, df_textcol = data$text
, unique_extr = F
, entity_type = 'cardinal'
, verbose = T)/data$WC
dim(data)
save(data
, data_unigrams
, data_bigrams
, data_trigrams
, data_unigrams_tfidf
, data_bigrams_tfidf
, data_trigrams_tfidf
, data_bigrams_plus
, data_trigrams_plus
, data_bigrams_plus_tfidf
, data_trigrams_plus_tfidf
, file = 'youtube_ads_feature_extraction.RData')
