title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
setwd("/Users/bennettkleinberg/Documents/Research/PhD Amsterdam/nwo_aanvraag")
hammer = 565
knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
citations = 4924
followers = 770
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 75 #based on google scholar
followers = 1273 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 8379 #based on google scholar
followers = 2240 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
library(equivalence)
df = data.frame('a' = rnorm(100, 2, 1))
df$b = rnorm(100, 1, 1)
mean(df$a)
mean(df$b)
library(equivalence)
df = data.frame('a' = rnorm(1000, 2, 1))
df$b = rnorm(1000, 1, 1)
mean(df$a)
mean(df$b)
df = data.frame('a' = rnorm(10000, 2, 1))
df$b = rnorm(10000, 1, 1)
mean(df$a)
mean(df$b)
t.test(df$a, df$b)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = .1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
mean(df$a)
mean(df$b)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
a = 1:5
b = 12:17
a
a*b
a = 1:5
b = 12:16
a*b
a%*%b
# clear ws
rm(list = ls())
# IC2S2 LSA Workshop Code
# 2017-07-10
# Jacob Miller, Drexel University: jlm479@drexel.edu
#    with assistance from Jorge Fresneda, David Gefen, James Endicott, Kai Larsen, and others in the R community
###########################
# Section 1: Set up R.
# If you have never loaded these packages in this R environment, you need to do this to download them.
# tm: Text Mining;
# LSAfun: LSA functions;
# Matrix: Sparse matrix package
# RSpectra: Fast SVD
# gplots: fancy plotting
#install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
rm(list = ls())
library(tm) # This loads them into your environment so you can now use that code.
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies = TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
library(gplots)
machFileDirectory = '/Users/bennettkleinberg/Documents/Research/analysis/lsa/files' # Mac/Unix style
# machFileDirectory = 'C:/Users/Owner/Desktop/Dropbox/LSA Workshop/files'
doc_source <- DirSource(machFileDirectory, recursive = TRUE)
object.size(doc_source)
raw_corpus <- VCorpus(doc_source, readerControl=list(language='en'))
omitDocNames = paste(c(452, 454, 456, 458, 460:470, 518), ".txt", sep="")
omitDocs = which(names(raw_corpus) %in% omitDocNames)
machRawtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
bounds=list(global=c(2,Inf))))
machtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
# stemming=TRUE,
stopwords = TRUE,
weighting = function(x)  weightTfIdf(x, normalize = FALSE),
bounds=list(global=c(2,Inf))))
machtdm
inspect(machtdm[1:20,1:8])
sparse_tdm <- Matrix::sparseMatrix(i = machtdm$i, j = machtdm$j, x = machtdm$v, dims = c(machtdm$nrow, machtdm$ncol))
dimnames(machtdm)
dimnames(sparse_tdm) <- dimnames(machtdm)
sparse_tdm[1:20,1:8]
machtdm
sparse_tdm[1:20,1:8]
class(machtdm)
class(sparse_tdm)
machtdm[1:20,1:8]
sparse_tdm[1:20,1:8]
dim(sparse_tdm) # What are the dimensions of the matrix? Rows, Columns
object.size(sparse_tdm) # How many bytes?
colnames(sparse_tdm) # What are the column names?
machRawtdm[1:20, 1:8]
machtdm
machtdm[1:20,1:8]
inspect(machRawtdm[1:20, 1:8])
inspect(machRawtdm[1:100, 1:8])
start=Sys.time() # This is a simple timing mechanism. Creates a variable that saves the current time and...
space <- svds(sparse_tdm, 100)
# 	easyspace = lsa(machtdm, dims=100) # This will give a warning - last dimension calculated is very close to zero size
Sys.time()-start # ... this line shows the now-current time minus the "start" time.
class(space) # A list
str(space) #          with five objects inside it
object.size(space)
dim(space)
tk <- space$d * space$u
dk <- space$d * Matrix::t(space$v)
dimnames(tk) <- list(dimnames(sparse_tdm)[[1]], 1:100)
dimnames(dk) <- list(1:100, dimnames(sparse_tdm)[[2]])
dim(tk) # Dimensions of term matrix
dim(dk) # Dimensions of doc matrix
length(space$d) # Number of singular values
tk[1:10,1:10]
round(tk[1:10,1:10], 1)
round(tk[1:10,1:10], 2)
findFreqTerms(machRawtdm, lowfreq=100) # Words which occur 100+ times
neighbors("trust", 30, tvectors=tk[,1:100])
neighbors("trust", 30, tvectors=tk[,1:30])
plot_neighbors("trust", 30, dims=2, tvectors=tk, breakdown=F)
plot_neighbors("trust", 30, dims=3, tvectors=tk, breakdown=F)
charLength = lapply(colnames(dk), nchar)
colnames(dk)[which(charLength==5)] = paste("00", colnames(dk)[which(charLength==5)], sep="")
colnames(dk)[which(charLength==6)] = paste("0", colnames(dk)[which(charLength==6)], sep="")
start=Sys.time()
paraCos = multicos(sort(colnames(dk)), tvectors=t(dk), breakdown=F)
Sys.time()-start
paraCos[1:10,1:10] # Look at first 10x10 of matrix.
write.csv(paraCos, file="Mach_paraCos.csv")
heatmap(paraCos, symm=T, main="Machiavelli Clustered Paras")
plot(space$d)
rTerms = sample(rownames(tk), 30) # random sample of term names
plot(tk[rTerms,1],tk[rTerms,2])
text(tk[rTerms,1],tk[rTerms,2], labels=rTerms)
costring("trust", "useful perfect", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "useful sin", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "elizabeth judgment", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince love", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince pikes", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state love", tvectors= tk[,1:50], breakdown=TRUE)
termlist = c("souldiours", "power", "prince", "princes", "state", "warre", "pikes", "enemies", "trust", "love", "fear", "pope", "alexander") # Create a list of specified terms to use for particular investigation.
specificTermCos = multicos(termlist, tvectors= tk[,1:50], breakdown=F)
specificTermCos
Term_count <-apply(tk,1,sum)
TCT <- t(Term_count)
myTerms <- rownames(tk)
str(myTerms)
TCT
plot_neighbors("prince", 30, dims=3, tvectors=tk, breakdown=F)
heatmap(paraCos, symm=T, Rowv=NA,  main="Machiavelli Ordered Paras")
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(stringr)
require(ez)
library(MASS)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
##########
#import custom AUC function
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_fold_average_auc.R')
#########
#set wd
setwd('/Users/bennettkleinberg/Documents/Research/outputfiles/ms_past_future/ml_merged')
################MACHINE LEARNING CLASSIFICATION################################
# FULL
##ML general: proportional increments
require(caret)
require(e1071)
require(pROC)
setwd('/Users/bennettkleinberg/Documents/Research/outputfiles/ms_past_future/ml_merged')
setwd('./percent_sub')
load('data_100_liwc.RData')
data = data_100
data = data[data$exp == 'exp2', ]
#prop.table(table(data$veracity))
data = data[,-c(1:33, 35:78)]
levels(data$veracity) = c(0, 1)
levels(data$veracity) = make.names(levels(data$veracity))
features_liwc_all = c(1, 2:94)
data_ml = data[, features_liwc_all]
set.seed(444)
in_training = createDataPartition(y = data_ml$veracity
, p = .8
, list = FALSE)
training_data = data_ml[ in_training,]
testing_data = data_ml[-in_training,]
controls = trainControl(method="repeatedcv"
, number=5
, repeats=10
, selectionFunction = "oneSE"
, classProbs = T
, summaryFunction = twoClassSummary
, savePredictions = T
)
svm_5k = train(veracity ~ .
, data = training_data
, method = "svmLinear"
, trControl = controls
, verbose = FALSE
, metric = 'ROC'
)
prop.table(table(data_ml$veracity))
testing_data$pred = predict(svm_5k, testing_data)
testing_data$probs = predict(svm_5k, testing_data, type = 'prob')
caret::confusionMatrix(testing_data$pred, testing_data$veracity)
setwd('/Users/bennettkleinberg/GitHub/youtube_ads_detection')
input_files = list.files(pattern = "*.txt")
input_files
input_files[1]
text_raw = readChar(input_files[1]
, file.info(input_files[1])$size)
text_raw
text_source = DirSource(input_files, recursive = TRUE)
library(tm)
library(data.table)
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
require(rJava)
library(qdap)
library(openNLP)
text_source = DirSource(input_files, recursive = TRUE)
output_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection/output_dir"
text_source = DirSource(output_dir, recursive = TRUE)
text_source
text_corpus = VCorpus(text_source
, readerControl=list(language='en'))
object.size(text_corpus)
tm_map(text_corpus, content_transformer(replace_contraction))
text_corpus
text_corpus$`2.txt`
text_corpus$`3.txt`
View(text_corpus$`3.txt`)
text_corpus
text_corpus[1]
?replace_number
text_raw_no_tags = gsub("<.*?>", "", text_raw)
text_raw_no_tags
text_raw = readChar(input_files[4]
, file.info(input_files[4])$size)
output_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection/output_dir"
setwd(output_dir)
input_files = list.files(pattern = "*.txt")
#input
text_raw = readChar(input_files[4]
, file.info(input_files[4])$size)
text_raw
text_raw_no_tags = gsub("<.*?>", "", text_raw)
text_raw_no_tags
text_raw_no_arrows = gsub("-->", "", text_raw_no_tags)
text_raw_no_arrows
gsub("\n.*?\n", "", text_raw_no_tags)
text_raw_no_tags = gsub("<.*?>", "", text_raw)
#remove arrows -->
text_raw_no_arrows = gsub("-->", "", text_raw_no_tags)
gsub("{d}", "", text_raw_no_arrows)
gsub("d{2}", "", text_raw_no_arrows)
gsub("d{1}", "", text_raw_no_arrows)
text_raw_no_tags = gsub("<.*?>", "", text_raw)
#remove arrows -->
text_raw_no_arrows = gsub("-->", "", text_raw_no_tags)
gsub("[.*?]", "", text_raw_no_arrows)
gsub("\\[.*?\\]", "", text_raw_no_arrows)
text_raw_no_brackets = gsub("\\[.*?\\]", "", text_raw_no_arrows)
gsub("\\n", "", text_raw_no_brackets)
gsub("[[:digit:]]", "", text_raw_no_newline)
text_raw_no_newline = gsub("\\n", "", text_raw_no_brackets)
gsub("[[:digit:]]", "", text_raw_no_newline)
gsub("[[:digit:]] :", "", text_raw_no_newline)
gsub("[[:digit:]]:", "", text_raw_no_newline)
gsub("[[:digit:]]{3}", "", text_raw_no_newline)
gsub("[[:digit:]]{3}:[[:digit:]]{2}:[[:digit:]]{2},[[:digit:]]{3}", "", text_raw_no_newline)
gsub("[[:digit:]]{2,}:[[:digit:]]{2}:[[:digit:]]{2},[[:digit:]]{3}", "", text_raw_no_newline)
text_raw_no_time = gsub("[[:digit:]]{2,}:[[:digit:]]{2}:[[:digit:]]{2},[[:digit:]]{3}", "", text_raw_no_newline)
text_raw_no_ws = trimws(text_raw_no_time)
text_raw_no_ws
gsub(" "{2,}, " ", text_raw_no_ws)
gsub(" {2,}", " ", text_raw_no_ws)
parent_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection"
setwd(parent_dir)
list.files('./output_dir', pattern = "*.txt")
source('./r_nlp/txt_df_from_dir.R')
source('./r_nlp/txt_df_from_dir.R')
transcripts = txt_df_from_dir(dirpath = './output_dir')
transcripts
gsub("<.*?>", "", transcripts$text)
clean_transcripts = function(input_column){
#remove tags
no_tags = gsub("<.*?>", "", input_column)
#remove arrows -->
no_arrows = gsub("-->", "", no_tags)
#remove text in brackets
no_brackets = gsub("\\[.*?\\]", "", no_arrows)
#remove newlines
no_newline = gsub("\\n", "", no_brackets)
#remove time indicator
no_time = gsub("[[:digit:]]{2,}:[[:digit:]]{2}:[[:digit:]]{2},[[:digit:]]{3}", "", no_newline)
#remove trailing and leading whitespace
no_ws = trimws(no_time)
#refine multiple whitespaces to one
fine_ws = gsub(" {2,}", " ", no_ws)
return(fine_ws)
}
transcripts$clean = clean_transcripts(transcripts$text)
View(transcripts)
clean_transcripts = function(input_column){
#remove tags
no_tags = gsub("<.*?>", "", input_column)
#remove arrows -->
no_arrows = gsub("-->", "", no_tags)
#remove text in brackets
no_brackets = gsub("\\[.*?\\]", "", no_arrows)
#remove newlines
no_newline = gsub("\\n", "", no_brackets)
#remove time indicator
no_time = gsub("[[:digit:]]{2,}:[[:digit:]]{2}:[[:digit:]]{2},[[:digit:]]{3}", "", no_newline)
#remove dashes
no_dash = gsub("-", "", no_time)
#remove trailing and leading whitespace
no_ws = trimws(no_dash)
#refine multiple whitespaces to one
fine_ws = gsub(" {2,}", " ", no_ws)
return(fine_ws)
}
transcripts$clean = clean_transcripts(transcripts$text)
View(transcripts)
#############################################################
### YOUTUBE TRANSCRIPT PARSER ###############################
### Kleinberg, Mozes, van der Vegt (ablphabetical order) ####
### https://github.com/ben-aaron188/youtube_ads_detection ###
#############################################################
#clear ws
rm(list = ls())
parent_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection"
setwd(parent_dir)
source('./r_nlp/txt_df_from_dir.R')
source('./r_nlp/clean_transcript.R')
transcripts = txt_df_from_dir(dirpath = './output_dir')
transcripts$clean = clean_transcripts(transcripts$text)
transcripts$clean = clean_transcript(transcripts$text)
View(transcripts)
