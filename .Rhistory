title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
hammer = 575
knife = 505
gun = 495
stick = 498
bat = 503
all = c(knife, hammer, gun, stick, bat)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 590),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,650,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
axis(side=2, at=seq(450,550,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
hammer = 555
knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 590),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,550,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 590),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,550,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
hammer = 565
knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 590),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,550,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
setwd("/Users/bennettkleinberg/Documents/Research/PhD Amsterdam/nwo_aanvraag")
hammer = 565
knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
citations = 4924
followers = 770
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 75 #based on google scholar
followers = 1273 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 8379 #based on google scholar
followers = 2240 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
library(equivalence)
df = data.frame('a' = rnorm(100, 2, 1))
df$b = rnorm(100, 1, 1)
mean(df$a)
mean(df$b)
library(equivalence)
df = data.frame('a' = rnorm(1000, 2, 1))
df$b = rnorm(1000, 1, 1)
mean(df$a)
mean(df$b)
df = data.frame('a' = rnorm(10000, 2, 1))
df$b = rnorm(10000, 1, 1)
mean(df$a)
mean(df$b)
t.test(df$a, df$b)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = .1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
mean(df$a)
mean(df$b)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
a = 1:5
b = 12:17
a
a*b
a = 1:5
b = 12:16
a*b
a%*%b
# clear ws
rm(list = ls())
# IC2S2 LSA Workshop Code
# 2017-07-10
# Jacob Miller, Drexel University: jlm479@drexel.edu
#    with assistance from Jorge Fresneda, David Gefen, James Endicott, Kai Larsen, and others in the R community
###########################
# Section 1: Set up R.
# If you have never loaded these packages in this R environment, you need to do this to download them.
# tm: Text Mining;
# LSAfun: LSA functions;
# Matrix: Sparse matrix package
# RSpectra: Fast SVD
# gplots: fancy plotting
#install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
rm(list = ls())
library(tm) # This loads them into your environment so you can now use that code.
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies = TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
library(gplots)
machFileDirectory = '/Users/bennettkleinberg/Documents/Research/analysis/lsa/files' # Mac/Unix style
# machFileDirectory = 'C:/Users/Owner/Desktop/Dropbox/LSA Workshop/files'
doc_source <- DirSource(machFileDirectory, recursive = TRUE)
object.size(doc_source)
raw_corpus <- VCorpus(doc_source, readerControl=list(language='en'))
omitDocNames = paste(c(452, 454, 456, 458, 460:470, 518), ".txt", sep="")
omitDocs = which(names(raw_corpus) %in% omitDocNames)
machRawtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
bounds=list(global=c(2,Inf))))
machtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
# stemming=TRUE,
stopwords = TRUE,
weighting = function(x)  weightTfIdf(x, normalize = FALSE),
bounds=list(global=c(2,Inf))))
machtdm
inspect(machtdm[1:20,1:8])
sparse_tdm <- Matrix::sparseMatrix(i = machtdm$i, j = machtdm$j, x = machtdm$v, dims = c(machtdm$nrow, machtdm$ncol))
dimnames(machtdm)
dimnames(sparse_tdm) <- dimnames(machtdm)
sparse_tdm[1:20,1:8]
machtdm
sparse_tdm[1:20,1:8]
class(machtdm)
class(sparse_tdm)
machtdm[1:20,1:8]
sparse_tdm[1:20,1:8]
dim(sparse_tdm) # What are the dimensions of the matrix? Rows, Columns
object.size(sparse_tdm) # How many bytes?
colnames(sparse_tdm) # What are the column names?
machRawtdm[1:20, 1:8]
machtdm
machtdm[1:20,1:8]
inspect(machRawtdm[1:20, 1:8])
inspect(machRawtdm[1:100, 1:8])
start=Sys.time() # This is a simple timing mechanism. Creates a variable that saves the current time and...
space <- svds(sparse_tdm, 100)
# 	easyspace = lsa(machtdm, dims=100) # This will give a warning - last dimension calculated is very close to zero size
Sys.time()-start # ... this line shows the now-current time minus the "start" time.
class(space) # A list
str(space) #          with five objects inside it
object.size(space)
dim(space)
tk <- space$d * space$u
dk <- space$d * Matrix::t(space$v)
dimnames(tk) <- list(dimnames(sparse_tdm)[[1]], 1:100)
dimnames(dk) <- list(1:100, dimnames(sparse_tdm)[[2]])
dim(tk) # Dimensions of term matrix
dim(dk) # Dimensions of doc matrix
length(space$d) # Number of singular values
tk[1:10,1:10]
round(tk[1:10,1:10], 1)
round(tk[1:10,1:10], 2)
findFreqTerms(machRawtdm, lowfreq=100) # Words which occur 100+ times
neighbors("trust", 30, tvectors=tk[,1:100])
neighbors("trust", 30, tvectors=tk[,1:30])
plot_neighbors("trust", 30, dims=2, tvectors=tk, breakdown=F)
plot_neighbors("trust", 30, dims=3, tvectors=tk, breakdown=F)
charLength = lapply(colnames(dk), nchar)
colnames(dk)[which(charLength==5)] = paste("00", colnames(dk)[which(charLength==5)], sep="")
colnames(dk)[which(charLength==6)] = paste("0", colnames(dk)[which(charLength==6)], sep="")
start=Sys.time()
paraCos = multicos(sort(colnames(dk)), tvectors=t(dk), breakdown=F)
Sys.time()-start
paraCos[1:10,1:10] # Look at first 10x10 of matrix.
write.csv(paraCos, file="Mach_paraCos.csv")
heatmap(paraCos, symm=T, main="Machiavelli Clustered Paras")
plot(space$d)
rTerms = sample(rownames(tk), 30) # random sample of term names
plot(tk[rTerms,1],tk[rTerms,2])
text(tk[rTerms,1],tk[rTerms,2], labels=rTerms)
costring("trust", "useful perfect", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "useful sin", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "elizabeth judgment", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince love", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince pikes", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state love", tvectors= tk[,1:50], breakdown=TRUE)
termlist = c("souldiours", "power", "prince", "princes", "state", "warre", "pikes", "enemies", "trust", "love", "fear", "pope", "alexander") # Create a list of specified terms to use for particular investigation.
specificTermCos = multicos(termlist, tvectors= tk[,1:50], breakdown=F)
specificTermCos
Term_count <-apply(tk,1,sum)
TCT <- t(Term_count)
myTerms <- rownames(tk)
str(myTerms)
TCT
plot_neighbors("prince", 30, dims=3, tvectors=tk, breakdown=F)
heatmap(paraCos, symm=T, Rowv=NA,  main="Machiavelli Ordered Paras")
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(stringr)
require(ez)
library(MASS)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#clear ws
rm(list = ls())
parent_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection"
setwd(parent_dir)
source('./r_nlp/txt_df_from_dir.R')
source('./r_nlp/clean_transcript.R')
transcripts = txt_df_from_dir(dirpath = './output_dir')
?str_extract
??str_extract
#############################################################
### YOUTUBE TRANSCRIPT PARSER ###############################
### Kleinberg, Mozes, van der Vegt (ablphabetical order) ####
### https://github.com/ben-aaron188/youtube_ads_detection ###
#############################################################
#clear ws
rm(list = ls())
parent_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection"
setwd(parent_dir)
#deps
source('./r_nlp/txt_df_from_dir.R')
source('./r_nlp/clean_transcript.R')
transcripts = txt_df_from_dir(dirpath = './output_dir')
View(transcripts)
transcripts$text_clean = clean_transcript(transcripts$text)
View(transcripts)
write.table(transcripts,
file='clean_transcripts_10102017.txt',
na = 'NA',
sep = '\t',
append=F,
row.names = F,
col.names = T
)
names(transcripts)
transcripts_clean = transcripts[, -1]
write.table(transcripts,
file='clean_transcripts_10102017.txt',
na = 'NA',
sep = '\t',
append=F,
row.names = F,
col.names = T
)
write.table(transcripts,
file='clean_transcripts_10102017.txt',
na = 'NA',
sep = '\t',
append=F,
row.names = F,
col.names = T
)
write.table(transcripts_clean,
file='clean_transcripts_10102017.txt',
na = 'NA',
sep = '\t',
append=F,
row.names = F,
col.names = T
)
wordcount <- function(str) {
sapply(gregexpr("\\b\\W+\\b", str, perl=TRUE), function(x) sum(x>0) ) + 1
}
wordcount("I am going")
#############################################################
### YOUTUBE TRANSCRIPT PARSER ###############################
### Kleinberg, Mozes, van der Vegt (ablphabetical order) ####
### https://github.com/ben-aaron188/youtube_ads_detection ###
#############################################################
#clear ws
rm(list = ls())
parent_dir = "/Users/bennettkleinberg/GitHub/youtube_ads_detection"
setwd(parent_dir)
#deps
source('./r_nlp/txt_df_from_dir.R')
source('./r_nlp/clean_transcript.R')
source('./r_nlp/wordcount.R')
transcripts = txt_df_from_dir(dirpath = './output_dir')
transcripts$text_clean = clean_transcript(transcripts$text)
names(transcripts)
transcripts_clean = transcripts[, -1]
transcripts_clean$nwords = wordcount(transcripts_clean$text_clean)
View(transcripts_clean)
sum(transcripts_clean$nwords)
write.table(transcripts_clean,
file='clean_transcripts_10102017.txt',
na = 'NA',
sep = '\t',
append=F,
row.names = F,
col.names = T
)
save(transcripts_clean
, file = 'clean_transcripts_10102017.RData')
